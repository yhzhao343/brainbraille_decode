{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9ad266-cdf8-4649-8a81-56dd38b46ec9",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def1a38e-34e6-4ea0-813f-668a11ecb3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhui/miniconda3/lib/python3.11/site-packages/lipo/hyperparameter.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import msgpack\n",
    "import msgpack_numpy as m\n",
    "import numpy as np\n",
    "from brainbraille_decode.cross_validation import BrainBrailleCVGen\n",
    "from brainbraille_decode.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    information_transfer_per_selection,\n",
    "    letter_label_to_word_label,\n",
    "    naive_information_transfer_per_selection,\n",
    "    tok_acc,\n",
    "    tok_corr,\n",
    ")\n",
    "from brainbraille_decode.preprocessing import (\n",
    "    ButterworthBandpassFilter,\n",
    "    DataSlice,\n",
    "    ROIandCalibrationExtractor,\n",
    ")\n",
    "from brainbraille_decode.viterbi_decoder import (\n",
    "    BrainBrailleDataToTransProbCV,\n",
    "    LetterProbaToLetterDecode,\n",
    "    StateProbaToLetterProb,\n",
    "    TransProbToStateProb,\n",
    "    letter_label_to_transition_label,\n",
    ")\n",
    "from fastFMRI.file_helpers import load_file, write_file\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "m.patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec28b6-74ad-4ef9-a9bb-ca0685d53123",
   "metadata": {},
   "source": [
    "# Load cached intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c1787f-f9b9-46e0-97e9-7328718316dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TO_USE = '3S'\n",
    "# FILE_TO_USE = '1S5_THAD'\n",
    "# FILE_TO_USE = '1S5_FEYI'\n",
    "\n",
    "info_dict = {\n",
    "    \"3S\": {\n",
    "        \"ENV_VAR\":\"BRAINBRAILLE_INTERMEDIATE_3S_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\":\"brainbraille_intermediate_3s_per_run_raw_data.bin\",\n",
    "        \"url\":\"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EYfSrpjQzcxKtVXkP1OeZrQBD6N1gNbiA3yqGjM6OhRoGg?e=3Vu9J1\"\n",
    "    },\n",
    "    \"1S5_THAD\": {\n",
    "        \"ENV_VAR\": \"BRAINBRAILLE_INTERMEDIATE_1S5_THAD_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\": \"brainbraille_intermediate_thad_1s5_run_raw_data.bin\",\n",
    "        \"url\": \"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EdR7OAnM9W5JjD2FcSxWYa4Bs9vtvU157Zug4QiKmHvTvQ?e=6mXYkN\"\n",
    "    },\n",
    "    \"1S5_FEYI\": {\n",
    "        \"ENV_VAR\": \"BRAINBRAILLE_INTERMEDIATE_1S5_FEYI_PER_RUN_RAW_DATA_FILE_PATH\",\n",
    "        \"file_name\": \"brainbraille_intermediate_feyi_1s5_run_raw_data.bin\",\n",
    "        \"url\": \"https://gtvault-my.sharepoint.com/:u:/g/personal/yzhao343_gatech_edu/EUMLUnsFKNNNhkxWtf6q5pkBvqDWYHM817GuBSRywYJZ8A?e=b0JQRf\"\n",
    "    }\n",
    "}\n",
    "\n",
    "ENV_VAR = info_dict[FILE_TO_USE][\"ENV_VAR\"]\n",
    "file_name = info_dict[FILE_TO_USE][\"file_name\"]\n",
    "url = info_dict[FILE_TO_USE][\"url\"]\n",
    "\n",
    "if ENV_VAR in os.environ:\n",
    "    brainbraille_intermediate_per_run_raw_data_file_path = os.environ[ENV_VAR]\n",
    "else:\n",
    "    brainbraille_intermediate_per_run_raw_data_file_path = file_name\n",
    "    if not os.path.exists(brainbraille_intermediate_per_run_raw_data_file_path):\n",
    "        from onedrivedownloader import download\n",
    "        # If the link expires, let me know and I will renew the link\n",
    "        download(\n",
    "            url=url,\n",
    "            filename=brainbraille_intermediate_per_run_raw_data_file_path,\n",
    "        )\n",
    "\n",
    "brainbraille_3s_per_run_data = msgpack.unpackb(\n",
    "    load_file(brainbraille_intermediate_per_run_raw_data_file_path, \"rb\"),\n",
    "    strict_map_key=False,\n",
    ")\n",
    "per_run_data = brainbraille_3s_per_run_data[\"per_run_data\"]\n",
    "grammar_info = brainbraille_3s_per_run_data[\"grammar_info\"]\n",
    "LETTERS_TO_DOT = brainbraille_3s_per_run_data[\"LETTERS_TO_DOT\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3147a12-0bbf-463b-9a4c-1fb183fef20c",
   "metadata": {},
   "source": [
    "# Print each run data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f894b20f-c56e-42d8-b9d6-ab04beb328ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subs: [1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 4 4 4 4]\n",
      "runs: [1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 1 2 1 2 3 4]\n",
      "sess: [1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "subs = np.array([info[\"sub\"] for info in per_run_data], dtype=int)\n",
    "runs = np.array([info[\"run\"] for info in per_run_data], dtype=int)\n",
    "sess = np.array([info[\"ses\"] for info in per_run_data], dtype=int)\n",
    "print(f\"subs: {np.array2string(subs, max_line_width=120)}\")\n",
    "print(f\"runs: {np.array2string(runs, max_line_width=120)}\")\n",
    "print(f\"sess: {np.array2string(sess, max_line_width=120)}\")\n",
    "cv_generator = BrainBrailleCVGen(subs, runs, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cdb34-a291-4a11-bc87-0a58b2fe8988",
   "metadata": {},
   "source": [
    "# Subject dependent experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3062c525-23e2-4a23-a3d1-281d39165ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SVM = True\n",
    "BANDPASS_LOW_CUT = 0.01\n",
    "BANDPASS_HIGH_CUT = 0.2\n",
    "BANDPASS_ORDER = 1\n",
    "DELAY_S = 3\n",
    "EXTRA_TIME_S = 3\n",
    "TR_s = per_run_data[0][\"TR_s\"]\n",
    "STIMULI_LABEL_INTERVAL_s = per_run_data[0][\"EVENT_INTERVAL_S\"]\n",
    "EXTRA_FRAME = int(EXTRA_TIME_S / TR_s)\n",
    "DELAY_FRAME = int(DELAY_S / TR_s)\n",
    "SF_Hz = 1 / TR_s\n",
    "NUM_FRAME_PER_LABEL = int(STIMULI_LABEL_INTERVAL_s / TR_s)\n",
    "EVENT_LEN_S = per_run_data[0][\"EVENT_LEN_S\"]\n",
    "EVENT_INTERVAL_S = per_run_data[0][\"EVENT_INTERVAL_S\"]\n",
    "EVENT_LEN_FRAME = int(EVENT_LEN_S / TR_s)\n",
    "EVENT_INTERVAL_FRAME = int(EVENT_INTERVAL_S / TR_s)\n",
    "regressor_types = per_run_data[0][\"regressor_types\"]\n",
    "inner_n_jobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fffa1473-8b96-4c00-b81c-dce938fc4ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10]), array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10]), array([ 0,  1,  2,  3,  4,  6,  7,  8,  9, 10]), array([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 10]), array([ 0,  1,  2,  3,  4,  5,  6,  8,  9, 10]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10]), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])]\n",
      "------------------------------------------------------\n",
      "[array([0]), array([1]), array([2]), array([3]), array([4]), array([5]), array([6]), array([7]), array([8]), array([9]), array([10])]\n"
     ]
    }
   ],
   "source": [
    "sub = 1\n",
    "# sub = 4\n",
    "n = 1\n",
    "train_index, test_index = cv_generator.sub_dependent_leave_n_run_out(sub=sub, n=n)\n",
    "print(train_index)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7779ed-ba7c-4ef2-88f9-a943cbbf39a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_i: 0\n",
      "train_runs:[ 1  2  3  4  5  6  7  8  9 10]\n",
      "test_runs:[0]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9900 - C: 990.378 gamma:   0.035\n",
      "region: f_l cost: 0.9758 - C:   5.486 gamma:   0.025\n",
      "region: f_r cost: 0.9810 - C:   3.739 gamma:   0.024\n",
      "region: h_l cost: 0.9827 - C: 888.873 gamma:   0.027\n",
      "region: h_r cost: 0.9831 - C:6608.872 gamma:   0.031\n",
      "region:   t cost: 0.9874 - C:   3.542 gamma:   0.033\n",
      "----- --------------- -----\n",
      "total time 33.045448541641235s\n",
      "--- naive letter 0.978\n",
      "fold_i: 1\n",
      "train_runs:[ 0  2  3  4  5  6  7  8  9 10]\n",
      "test_runs:[1]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9896 - C:   2.690 gamma:   0.028\n",
      "region: f_l cost: 0.9753 - C: 303.029 gamma:   0.034\n",
      "region: f_r cost: 0.9827 - C:  29.440 gamma:   0.024\n",
      "region: h_l cost: 0.9810 - C: 971.166 gamma:   0.025\n",
      "region: h_r cost: 0.9835 - C:  98.376 gamma:   0.031\n",
      "region:   t cost: 0.9866 - C:  14.677 gamma:   0.029\n",
      "----- --------------- -----\n",
      "total time 32.986830711364746s\n",
      "--- naive letter 0.961\n",
      "fold_i: 2\n",
      "train_runs:[ 0  1  3  4  5  6  7  8  9 10]\n",
      "test_runs:[2]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9866 - C:  80.846 gamma:   0.033\n",
      "region: f_l cost: 0.9775 - C:   9.909 gamma:   0.030\n",
      "region: f_r cost: 0.9792 - C:  35.904 gamma:   0.033\n",
      "region: h_l cost: 0.9814 - C: 200.732 gamma:   0.025\n",
      "region: h_r cost: 0.9831 - C:  31.343 gamma:   0.022\n",
      "region:   t cost: 0.9840 - C:  20.722 gamma:   0.030\n",
      "----- --------------- -----\n",
      "total time 29.086371660232544s\n",
      "--- naive letter 0.978\n",
      "fold_i: 3\n",
      "train_runs:[ 0  1  2  4  5  6  7  8  9 10]\n",
      "test_runs:[3]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9870 - C:  13.951 gamma:   0.031\n",
      "region: f_l cost: 0.9758 - C: 331.391 gamma:   0.031\n",
      "region: f_r cost: 0.9792 - C:   2.861 gamma:   0.020\n",
      "region: h_l cost: 0.9805 - C:  81.813 gamma:   0.025\n",
      "region: h_r cost: 0.9823 - C:17878.762 gamma:   0.035\n",
      "region:   t cost: 0.9853 - C:  32.094 gamma:   0.033\n",
      "----- --------------- -----\n",
      "total time 33.27496099472046s\n",
      "--- naive letter 0.961\n",
      "fold_i: 4\n",
      "train_runs:[ 0  1  2  3  5  6  7  8  9 10]\n",
      "test_runs:[4]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9892 - C:  14.949 gamma:   0.031\n",
      "region: f_l cost: 0.9788 - C: 755.517 gamma:   0.028\n",
      "region: f_r cost: 0.9814 - C:  16.591 gamma:   0.020\n",
      "region: h_l cost: 0.9827 - C: 456.086 gamma:   0.026\n",
      "region: h_r cost: 0.9844 - C: 296.667 gamma:   0.026\n",
      "region:   t cost: 0.9874 - C: 320.048 gamma:   0.027\n",
      "----- --------------- -----\n",
      "total time 29.312509059906006s\n",
      "--- naive letter 0.948\n",
      "fold_i: 5\n",
      "train_runs:[ 0  1  2  3  4  6  7  8  9 10]\n",
      "test_runs:[5]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9840 - C:   1.320 gamma:   0.030\n",
      "region: f_l cost: 0.9745 - C: 439.391 gamma:   0.029\n",
      "region: f_r cost: 0.9810 - C: 152.160 gamma:   0.022\n",
      "region: h_l cost: 0.9784 - C: 301.901 gamma:   0.032\n",
      "region: h_r cost: 0.9792 - C: 332.187 gamma:   0.028\n",
      "region:   t cost: 0.9831 - C:  31.623 gamma:   0.022\n",
      "----- --------------- -----\n",
      "total time 29.861453771591187s\n",
      "--- naive letter 0.978\n",
      "fold_i: 6\n",
      "train_runs:[ 0  1  2  3  4  5  7  8  9 10]\n",
      "test_runs:[6]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9905 - C:   1.583 gamma:   0.027\n",
      "region: f_l cost: 0.9797 - C:   4.473 gamma:   0.027\n",
      "region: f_r cost: 0.9814 - C:   1.380 gamma:   0.027\n",
      "region: h_l cost: 0.9840 - C:   4.175 gamma:   0.023\n",
      "region: h_r cost: 0.9848 - C:   8.014 gamma:   0.032\n",
      "region:   t cost: 0.9861 - C:  13.384 gamma:   0.030\n",
      "----- --------------- -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhui/miniconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 31.637319803237915s\n",
      "--- naive letter 0.974\n",
      "fold_i: 7\n",
      "train_runs:[ 0  1  2  3  4  5  6  8  9 10]\n",
      "test_runs:[7]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9913 - C:  21.327 gamma:   0.030\n",
      "region: f_l cost: 0.9784 - C: 969.523 gamma:   0.029\n",
      "region: f_r cost: 0.9831 - C:  31.623 gamma:   0.022\n",
      "region: h_l cost: 0.9797 - C: 949.297 gamma:   0.019\n",
      "region: h_r cost: 0.9814 - C: 140.895 gamma:   0.030\n",
      "region:   t cost: 0.9844 - C:   4.598 gamma:   0.034\n",
      "----- --------------- -----\n",
      "total time 30.67418122291565s\n",
      "--- naive letter 0.978\n",
      "fold_i: 8\n",
      "train_runs:[ 0  1  2  3  4  5  6  7  9 10]\n",
      "test_runs:[8]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9913 - C: 295.846 gamma:   0.032\n",
      "region: f_l cost: 0.9788 - C: 154.597 gamma:   0.031\n",
      "region: f_r cost: 0.9797 - C:  91.229 gamma:   0.027\n",
      "region: h_l cost: 0.9844 - C:  81.556 gamma:   0.023\n",
      "region: h_r cost: 0.9857 - C: 888.486 gamma:   0.027\n",
      "region:   t cost: 0.9848 - C:  45.531 gamma:   0.027\n",
      "----- --------------- -----\n",
      "total time 30.815319061279297s\n",
      "--- naive letter 0.935\n",
      "fold_i: 9\n",
      "train_runs:[ 0  1  2  3  4  5  6  7  8 10]\n",
      "test_runs:[9]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9905 - C:   2.751 gamma:   0.022\n",
      "region: f_l cost: 0.9810 - C:  75.165 gamma:   0.035\n",
      "region: f_r cost: 0.9848 - C:  18.528 gamma:   0.019\n",
      "region: h_l cost: 0.9831 - C:  16.591 gamma:   0.020\n",
      "region: h_r cost: 0.9848 - C: 333.783 gamma:   0.029\n",
      "region:   t cost: 0.9879 - C:  32.450 gamma:   0.025\n",
      "----- --------------- -----\n",
      "total time 30.739871978759766s\n",
      "--- naive letter 0.957\n",
      "fold_i: 10\n",
      "train_runs:[0 1 2 3 4 5 6 7 8 9]\n",
      "test_runs:[10]\n",
      "----- tune_SVM_result -----\n",
      "region:   b cost: 0.9883 - C:  31.623 gamma:   0.022\n",
      "region: f_l cost: 0.9784 - C:6907.287 gamma:   0.030\n",
      "region: f_r cost: 0.9835 - C:   2.674 gamma:   0.024\n",
      "region: h_l cost: 0.9823 - C:  21.985 gamma:   0.026\n",
      "region: h_r cost: 0.9805 - C: 140.166 gamma:   0.030\n",
      "region:   t cost: 0.9874 - C: 929.279 gamma:   0.030\n",
      "----- --------------- -----\n",
      "total time 30.85273838043213s\n",
      "--- naive letter 0.978\n"
     ]
    }
   ],
   "source": [
    "if RUN_SVM:\n",
    "    pred_trans_label_by_type = {}\n",
    "    y_trans_label_by_type = {}\n",
    "\n",
    "    for r_i, r in enumerate(regressor_types):\n",
    "        if r not in pred_trans_label_by_type:\n",
    "            pred_trans_label_by_type[r] = []\n",
    "        if r not in y_trans_label_by_type:\n",
    "            y_trans_label_by_type[r] = []\n",
    "\n",
    "    test_label_list = []\n",
    "    naive_prob_letter_label_list = []\n",
    "    naive_cm_list = []\n",
    "\n",
    "    stimulus_letter_viterbi_cm_list = []\n",
    "    mackenzie_soukoreff_letter_viterbi_cm_list = []\n",
    "\n",
    "    stimulus_pred_y_list = []\n",
    "\n",
    "    stimulus_pred_bigram_weighted_letter_label_list = []\n",
    "    stimulus_pred_letter_viterbi_decode_letter_label_list = []\n",
    "\n",
    "    aw2aw_stimulus_pred_y_list = []\n",
    "    aw2aw_stimulus_pred_bigram_weighted_letter_label_list = []\n",
    "    aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list = []\n",
    "\n",
    "    mackenzie_soukoreff_pred_y_list = []\n",
    "    mackenzie_soukoreff_pred_bigram_weighted_letter_label_list = []\n",
    "    mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list = []\n",
    "\n",
    "    aw2aw_mackenzie_soukoreff_pred_y_list = []\n",
    "    aw2aw_mackenzie_soukoreff_pred_bigram_weighted_letter_label_list = []\n",
    "    aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list = []\n",
    "\n",
    "    train_index, test_index = cv_generator.sub_dependent_leave_n_run_out(sub=sub, n=n)\n",
    "\n",
    "    for fold_i, (train_i, test_i) in enumerate(zip(train_index, test_index)):\n",
    "        print(\n",
    "            f\"fold_i: {fold_i}\\ntrain_runs:{np.array2string(train_i, 120)}\\ntest_runs:{np.array2string(test_i, 120)}\"\n",
    "        )\n",
    "        s_time = time.time()\n",
    "        train_data = [per_run_data[i] for i in train_i]\n",
    "        train_label = [d_i[\"letter_label\"] for d_i in train_data]\n",
    "        test_data = [per_run_data[i] for i in test_i]\n",
    "        test_label = [d_i[\"letter_label\"] for d_i in test_data]\n",
    "        test_label_list += test_label\n",
    "        train_sub_i = [int(train_data_i[\"sub\"]) for train_data_i in train_data]\n",
    "        test_sub_i = [int(test_data_i[\"sub\"]) for test_data_i in test_data]\n",
    "        test_label_flatten = [l for run in test_label for l in run]\n",
    "        \n",
    "        roi_and_calib_extractor = ROIandCalibrationExtractor()\n",
    "\n",
    "        data_slicer = DataSlice(\n",
    "            EXTRA_FRAME,\n",
    "            DELAY_FRAME,\n",
    "            EVENT_LEN_FRAME,\n",
    "            EVENT_INTERVAL_FRAME,\n",
    "        )\n",
    "        svc_param = {\n",
    "            \"C\": [1.0, 1000.0],\n",
    "            \"gamma\": [0.01, 0.05],\n",
    "        }\n",
    "\n",
    "        svm_param_transform = {}\n",
    "        log_args_keys = [\"C\", \"gamma\"]\n",
    "\n",
    "        butter_filter = ButterworthBandpassFilter(\n",
    "            BANDPASS_LOW_CUT,\n",
    "            BANDPASS_HIGH_CUT,\n",
    "            SF_Hz,\n",
    "            BANDPASS_ORDER,\n",
    "        )\n",
    "\n",
    "        convert_to_trans_prob_CV = BrainBrailleDataToTransProbCV(\n",
    "            LETTERS_TO_DOT,\n",
    "            regressor_types,\n",
    "            SVC(kernel=\"rbf\", cache_size=2000, break_ties=True, class_weight=None),\n",
    "            data_slicer,\n",
    "            svc_param,\n",
    "            param_category_keys=param_category_keys,\n",
    "            log_args_keys=log_args_keys,\n",
    "            param_transform_dict=svm_param_transform,\n",
    "            train_group=train_sub_i,\n",
    "            test_group=test_sub_i,\n",
    "            z_normalize=True,\n",
    "            n_calls=50,\n",
    "            inner_n_jobs=inner_n_jobs,\n",
    "        )\n",
    "\n",
    "        trans_prob_to_state_prob = TransProbToStateProb(LETTERS_TO_DOT, regressor_types)\n",
    "        state_prob_to_letter_prob = StateProbaToLetterProb(\n",
    "            LETTERS_TO_DOT, regressor_types\n",
    "        )\n",
    "\n",
    "        viterbi_decoder = LetterProbaToLetterDecode(\n",
    "            LETTERS_TO_DOT,\n",
    "            regressor_types,\n",
    "            bigram_dict=grammar_info[\"stimulus_letter_bigram_prob_dict\"],\n",
    "            words_node_symbols=grammar_info[\"stimulus_words_node_symbols\"],\n",
    "            words_link_start_end=grammar_info[\"stimulus_words_link_start_end\"],\n",
    "            words_dictionary=grammar_info[\"unique_stimulus_word_dictionary\"],\n",
    "            insertion_penalty=0,\n",
    "            insertion_penalty_lower=-10.0,\n",
    "            insertion_penalty_higher=10.0,\n",
    "            softmax_on_bigram_matrix=True,\n",
    "            CV_tune_insertion_penalty=True,\n",
    "            skip_letter_viterbi=False,\n",
    "            skip_grammar_viterbi=False,\n",
    "            random_state=42,\n",
    "            n_calls=32,\n",
    "        )\n",
    "\n",
    "        svm_cv_pipe = Pipeline(\n",
    "            steps=[\n",
    "                (\"data_extractor\", roi_and_calib_extractor),\n",
    "                (\"bandpass\", butter_filter),\n",
    "                (\"transition_probability_CV\", convert_to_trans_prob_CV),\n",
    "                (\"transition_to_state_prob\", trans_prob_to_state_prob),\n",
    "                (\"state_to_letter_prob\", state_prob_to_letter_prob),\n",
    "                (\"viterbi_decoder\", viterbi_decoder),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fit_start_time = time.time()\n",
    "        svm_cv_pipe.fit(train_data, train_label)\n",
    "        # svm_cv_pipe.transform(test_data)\n",
    "        test_predict_letter = svm_cv_pipe.predict(test_data)\n",
    "        fit_end_time = time.time()\n",
    "        print(f\"total time {fit_end_time - fit_start_time}s\")\n",
    "\n",
    "        naive_cm_list.append(state_prob_to_letter_prob.get_naive_letter_cm(test_label))\n",
    "        test_trans_class = letter_label_to_transition_label(\n",
    "            test_label, LETTERS_TO_DOT, regressor_types\n",
    "        )\n",
    "        pred_trans_class = convert_to_trans_prob_CV.get_trans_class()\n",
    "        naive_acc = np.diag(naive_cm_list[-1]).sum() / naive_cm_list[-1].sum()\n",
    "        print(f\"--- naive letter {naive_acc:.3f}\")\n",
    "        \n",
    "        for r_i, r in enumerate(regressor_types):\n",
    "            pred_trans_label_by_type[r] += [\n",
    "                item[r_i] for run in pred_trans_class for item in run\n",
    "            ]\n",
    "            y_trans_label_by_type[r] += [\n",
    "                item[r_i] for run in test_trans_class for item in run\n",
    "            ]\n",
    "\n",
    "        stimulus_letter_viterbi_cm_list.append(\n",
    "            viterbi_decoder.obtain_letter_viterbi_cm(test_label)\n",
    "        )\n",
    "        stimulus_pred_y_list += test_predict_letter\n",
    "        naive_prob_letter_label_list += viterbi_decoder.naive_prob_letter_label\n",
    "        stimulus_pred_bigram_weighted_letter_label_list += (\n",
    "            viterbi_decoder.bigram_weighted_letter_label\n",
    "        )\n",
    "        stimulus_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            viterbi_decoder.letter_viterbi_decode_letter_label\n",
    "        )\n",
    "        print(\n",
    "            f\"--- stimulus {accuracy_score(test_label_flatten, [l for run in test_predict_letter for l in run]):.3f}\"\n",
    "        )\n",
    "\n",
    "        viterbi_decoder.re_tune(\n",
    "            bigram_dict=grammar_info[\"stimulus_letter_bigram_prob_dict\"],\n",
    "            words_node_symbols=grammar_info[\"aw2aw_stimulus_words_node_symbols\"],\n",
    "            words_link_start_end=grammar_info[\"aw2aw_stimulus_words_link_start_end\"],\n",
    "            words_dictionary=grammar_info[\"unique_stimulus_word_dictionary\"],\n",
    "        )\n",
    "\n",
    "        aw2aw_stimulus_pred_y_fold_i = viterbi_decoder.predict() \n",
    "        aw2aw_stimulus_pred_y_list += aw2aw_stimulus_pred_y_fold_i\n",
    "        aw2aw_stimulus_pred_bigram_weighted_letter_label_list += (\n",
    "            viterbi_decoder.bigram_weighted_letter_label\n",
    "        )\n",
    "        aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            viterbi_decoder.letter_viterbi_decode_letter_label\n",
    "        )\n",
    "        mackenzie_soukoreff_letter_viterbi_cm_list.append(\n",
    "            viterbi_decoder.obtain_letter_viterbi_cm(test_label)\n",
    "        )\n",
    "        print(\n",
    "            f\"--- aw2aw_stimulus {accuracy_score(test_label_flatten, [l for run in aw2aw_stimulus_pred_y_fold_i for l in run]):.3f}\"\n",
    "        )\n",
    "\n",
    "        viterbi_decoder.re_tune(\n",
    "            bigram_dict=grammar_info[\"mackenzie_soukoreff_letter_bigram_prob_dict\"],\n",
    "            words_node_symbols=grammar_info[\"mackenzie_soukoreff_words_node_symbols\"],\n",
    "            words_link_start_end=grammar_info[\n",
    "                \"mackenzie_soukoreff_words_link_start_end\"\n",
    "            ],\n",
    "            words_dictionary=grammar_info[\"unique_mackenzie_soukoreff_word_dictionary\"],\n",
    "        )\n",
    "        mackenzie_soukoreff_pred_y_fold_i = viterbi_decoder.predict()\n",
    "        mackenzie_soukoreff_pred_y_list += mackenzie_soukoreff_pred_y_fold_i\n",
    "        mackenzie_soukoreff_pred_bigram_weighted_letter_label_list += (\n",
    "            viterbi_decoder.bigram_weighted_letter_label\n",
    "        )\n",
    "        mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            viterbi_decoder.letter_viterbi_decode_letter_label\n",
    "        )\n",
    "        print(\n",
    "            f\"--- mackenzie_soukoreff {accuracy_score(test_label_flatten, [l for run in mackenzie_soukoreff_pred_y_fold_i for l in run]):.3f}\"\n",
    "        )\n",
    "\n",
    "        viterbi_decoder.re_tune(\n",
    "            bigram_dict=grammar_info[\"mackenzie_soukoreff_letter_bigram_prob_dict\"],\n",
    "            words_node_symbols=grammar_info[\n",
    "                \"aw2aw_mackenzie_soukoreff_words_node_symbols\"\n",
    "            ],\n",
    "            words_link_start_end=grammar_info[\n",
    "                \"aw2aw_mackenzie_soukoreff_words_link_start_end\"\n",
    "            ],\n",
    "            words_dictionary=grammar_info[\"unique_mackenzie_soukoreff_word_dictionary\"],\n",
    "        )\n",
    "        aw2aw_mackenzie_soukoreff_pred_y_fold_i = viterbi_decoder.predict()\n",
    "        aw2aw_mackenzie_soukoreff_pred_y_list += aw2aw_mackenzie_soukoreff_pred_y_fold_i\n",
    "        aw2aw_mackenzie_soukoreff_pred_bigram_weighted_letter_label_list += (\n",
    "            viterbi_decoder.bigram_weighted_letter_label\n",
    "        )\n",
    "        aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list += (\n",
    "            viterbi_decoder.letter_viterbi_decode_letter_label\n",
    "        )\n",
    "        print(\n",
    "            f\"--- mackenzie_soukoreff aw2aw {accuracy_score(test_label_flatten, [l for run in aw2aw_mackenzie_soukoreff_pred_y_fold_i for l in run]):.3f}\"\n",
    "        )\n",
    "        e_time = time.time()\n",
    "        pred_y_str = [e for run_i in test_predict_letter for e in run_i]\n",
    "        test_label_str = [e for run_i in test_label for e in run_i]\n",
    "        print(\n",
    "            f'fold: {fold_i} time: {e_time - s_time}s acc: {accuracy_score(test_label_str, pred_y_str)}\\n{\"\".join(pred_y_str)}\\n{\"\".join(test_label_str)}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa1f522-1de8-42c1-844c-b0deaaa2733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_result_cache_path = f'./brainbraille_SVM_result_cache_{FILE_TO_USE}_{STIMULI_LABEL_INTERVAL_s}s_sub_{sub}_test_size_{n}_{datetime.utcnow().strftime(\"%m_%d_%H_%M_%S\")}.bin'\n",
    "if RUN_SVM:\n",
    "    cached_results = {\n",
    "        \"pred_trans_label_by_type\": pred_trans_label_by_type,\n",
    "        \"y_trans_label_by_type\": y_trans_label_by_type,\n",
    "        \"test_label_list\": test_label_list,\n",
    "        \"naive_prob_letter_label_list\": naive_prob_letter_label_list,\n",
    "        \"naive_cm_list\": naive_cm_list,\n",
    "        \"stimulus_letter_viterbi_cm_list\": stimulus_letter_viterbi_cm_list,\n",
    "        \"mackenzie_soukoreff_letter_viterbi_cm_list\": mackenzie_soukoreff_letter_viterbi_cm_list,\n",
    "        \"stimulus_pred_y_list\": stimulus_pred_y_list,\n",
    "        \"stimulus_pred_bigram_weighted_letter_label_list\": stimulus_pred_bigram_weighted_letter_label_list,\n",
    "        \"stimulus_pred_letter_viterbi_decode_letter_label_list\": stimulus_pred_letter_viterbi_decode_letter_label_list,\n",
    "        \"aw2aw_stimulus_pred_y_list\": aw2aw_stimulus_pred_y_list,\n",
    "        \"aw2aw_stimulus_pred_bigram_weighted_letter_label_list\": aw2aw_stimulus_pred_bigram_weighted_letter_label_list,\n",
    "        \"aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list\": aw2aw_stimulus_pred_letter_viterbi_decode_letter_label_list,\n",
    "        \"mackenzie_soukoreff_pred_y_list\": mackenzie_soukoreff_pred_y_list,\n",
    "        \"mackenzie_soukoreff_pred_bigram_weighted_letter_label_list\": mackenzie_soukoreff_pred_bigram_weighted_letter_label_list,\n",
    "        \"mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list\": mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list,\n",
    "        \"aw2aw_mackenzie_soukoreff_pred_y_list\": aw2aw_mackenzie_soukoreff_pred_y_list,\n",
    "        \"aw2aw_mackenzie_soukoreff_pred_bigram_weighted_letter_label_list\": aw2aw_mackenzie_soukoreff_pred_bigram_weighted_letter_label_list,\n",
    "        \"aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list\": aw2aw_mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list,\n",
    "    }\n",
    "    write_file(\n",
    "        msgpack.packb(cached_results, use_bin_type=True), SVM_result_cache_path, \"wb\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a7fd8-2733-4c40-89af-02d8f736bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 8\n",
    "# print(accuracy_score(test_label_list[i], pred_y_list[i]))\n",
    "# if RUN_SVM or USE_SVM_RESULT_CACHE:\n",
    "delimiter = \"\\t\"\n",
    "grammar_decode_ITR = grammar_info[\"grammar_decode_ITR\"]\n",
    "\n",
    "\n",
    "# delimiter = ','\n",
    "def pp_array(arr, delimiter=\"\\t\"):\n",
    "    print(\n",
    "        np.array2string(\n",
    "            np.array(arr),\n",
    "            max_line_width=160,\n",
    "            separator=delimiter,\n",
    "            formatter={\"float_kind\": lambda x: f\"{x:6.4f}\"},\n",
    "        )[1:-1]\n",
    "    )\n",
    "\n",
    "\n",
    "region_accuracies, confusion_matrices = zip(\n",
    "    *[\n",
    "        (\n",
    "            accuracy_score(y_trans_label_by_type[r], pred_trans_label_by_type[r]),\n",
    "            confusion_matrix(y_trans_label_by_type[r], pred_trans_label_by_type[r]),\n",
    "        )\n",
    "        for r_i, r in enumerate(regressor_types)\n",
    "    ]\n",
    ")\n",
    "print(\"\\t\".join(regressor_types))\n",
    "print(\"\\t\".join([f\"{acc:.4f}\" for acc in region_accuracies]))\n",
    "#   print(regressor_types)\n",
    "#   print(np.array_str(np.array(region_accuracies), precision=4, suppress_small=True))\n",
    "\n",
    "for r, m in zip(regressor_types, confusion_matrices):\n",
    "    print(f\"\\n{r}\")\n",
    "    print(\"\\n\".join([delimiter.join([f\"{w:4}\" for w in line]) for line in m]))\n",
    "\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, naive_prob_letter_label_list)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n====================Naive letter accuracy====================\")\n",
    "naive_acc = np.mean(accuracy_list)\n",
    "naive_std = np.std(accuracy_list, ddof=1)\n",
    "print(f\"naive accuracy:{naive_acc:6.4f} std:{naive_std:6.4f}\\n\")\n",
    "# info_per_select =  [information_transfer_per_selection(mackenzie_soukoreff_letter_prior_prob_dict, cm) for cm in naive_cm_list]\n",
    "all_label = [l for run in test_label_list for l in run]\n",
    "num_label = np.unique(all_label).size\n",
    "all_naive_pred = [l for run in naive_prob_letter_label_list for l in run]\n",
    "# cm = np.sum(naive_cm_list, axis=0)\n",
    "cm_list = [\n",
    "    confusion_matrix(label, pred_y)\n",
    "    for label, pred_y in zip(test_label_list, naive_prob_letter_label_list)\n",
    "]\n",
    "cm = np.sum(cm_list, axis=0)\n",
    "# letter_cm = np.sum( , axis=0)\n",
    "naive_info_per_select = naive_information_transfer_per_selection(\n",
    "    num_label, accuracy_score(all_label, all_naive_pred)\n",
    ")\n",
    "better_info_per_select = information_transfer_per_selection(\n",
    "    grammar_info[\"mackenzie_soukoreff_letter_prior_prob_dict\"], cm\n",
    ")\n",
    "naive_ITR = 60 / STIMULI_LABEL_INTERVAL_s * naive_info_per_select\n",
    "better_ITR = 60 / STIMULI_LABEL_INTERVAL_s * better_info_per_select\n",
    "print(\n",
    "    f\" Naive information transfer per selection: {naive_info_per_select:6.4f} ITR: {naive_ITR:6.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Better information transfer per selection: {better_info_per_select:6.4f} ITR: {better_ITR:6.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "# print(f'{naive_acc:6.4f}\\t{naive_std:6.4f}\\t{naive_ITR:6.4f}\\t{better_ITR:6.4f}')\n",
    "# print(delimiter.join(np.array2string))\n",
    "# print(np.array2string(\n",
    "#         np.array(),\n",
    "#         separator=delimiter,\n",
    "#         formatter={'float_kind': lambda x: f'{x:6.4f}'}\n",
    "#     )[1:-1]\n",
    "# )\n",
    "print(\"-----------\")\n",
    "pp_array([naive_acc, naive_std, naive_ITR, better_ITR], delimiter)\n",
    "# print(f'naive info_per_select: {np.mean(info_per_select)}\\t{np.std(info_per_select)}')\n",
    "# bad_info_per_select = [naive_information_transfer_per_selection(27, acc) for acc in accuracy_list]\n",
    "# print(f'naive info_per_select: {np.mean(bad_info_per_select)}\\t{np.std(bad_info_per_select)}')\n",
    "# print(classification_report(test_label_list[0], naive_prob_letter_label_list[0]))\n",
    "\n",
    "print(\"\\n====================stim_9 letter grammar====================\")\n",
    "\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(\n",
    "            test_label_list, stimulus_pred_bigram_weighted_letter_label_list\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "bigram_acc = np.mean(accuracy_list)\n",
    "bigram_std = np.std(accuracy_list, ddof=1)\n",
    "print(f\"bigram:\\n{bigram_acc:6.4f}\\t{bigram_std:6.4f}\")\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(\n",
    "            test_label_list, stimulus_pred_letter_viterbi_decode_letter_label_list\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "letter_viterbi_mean_acc = np.mean(accuracy_list)\n",
    "letter_viterbi_mean_std = np.std(accuracy_list, ddof=1)\n",
    "print(\n",
    "    f\"letter viterbi accuracy: {letter_viterbi_mean_acc:6.4f}\\tstd:{letter_viterbi_mean_std:6.4f}\"\n",
    ")\n",
    "# info_per_select =  [information_transfer_per_selection(stimulus_letter_prior_prob_dict, cm) for cm in stimulus_letter_viterbi_cm_list]\n",
    "# cm = np.sum(stimulus_letter_viterbi_cm_list, axis=0)\n",
    "cm_list = [\n",
    "    confusion_matrix(label, pred_y)\n",
    "    for label, pred_y in zip(\n",
    "        test_label_list, stimulus_pred_letter_viterbi_decode_letter_label_list\n",
    "    )\n",
    "]\n",
    "cm = np.sum(cm_list, axis=0)\n",
    "naive_info_per_select = naive_information_transfer_per_selection(\n",
    "    num_label, letter_viterbi_mean_acc\n",
    ")\n",
    "better_info_per_select = information_transfer_per_selection(\n",
    "    grammar_info[\"stimulus_letter_prior_prob_dict\"], cm\n",
    ")\n",
    "naive_ITR = 60 / STIMULI_LABEL_INTERVAL_s * naive_info_per_select\n",
    "better_ITR = 60 / STIMULI_LABEL_INTERVAL_s * better_info_per_select\n",
    "print(\n",
    "    f\" Naive letter viterbi information transfer per selection: {naive_info_per_select:6.4f} ITR: {naive_ITR:6.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Better letter viterbi information transfer per selection: {better_info_per_select:6.4f} ITR: {better_ITR:6.4f}\"\n",
    ")\n",
    "print(\"-----------\")\n",
    "pp_array(\n",
    "    [\n",
    "        bigram_acc,\n",
    "        bigram_std,\n",
    "        letter_viterbi_mean_acc,\n",
    "        letter_viterbi_mean_std,\n",
    "        naive_ITR,\n",
    "        better_ITR,\n",
    "    ],\n",
    "    delimiter,\n",
    ")\n",
    "\n",
    "print(\"\\n====================stim_9 SFFW grammar====================\")\n",
    "print(\"\\ngrammar viterbi:\")\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, stimulus_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, stimulus_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, stimulus_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    np.array2string(accuracy_list, max_line_width=120, precision=4, suppress_small=True)\n",
    ")\n",
    "clf_acc = np.mean(accuracy_list)\n",
    "clf_std = np.std(accuracy_list, ddof=1)\n",
    "stim_sffw_ITR = grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"stim\"][\"SFFW\"] * clf_acc\n",
    "print(\n",
    "    f\"classification result:\\naccuracy:{clf_acc:6.4f} {clf_std:6.4f} ITR:{stim_sffw_ITR:6.4f}\"\n",
    ")\n",
    "print(\"-----------\")\n",
    "l_acc = np.mean(acc_list)\n",
    "l_acc_std = np.std(acc_list, ddof=1)\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"letter acc:\\n{l_acc:6.4f} {l_acc_std:6.4f}\\n\")\n",
    "l_corr = np.mean(corr_list)\n",
    "l_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"letter corr:\\n{l_corr:6.4f} {l_corr_std:6.4f}\")\n",
    "print(\"-----------\")\n",
    "test_label_word_list = [\n",
    "    letter_label_to_word_label(pred) for pred in stimulus_pred_y_list\n",
    "]\n",
    "pred_y_word_list = [letter_label_to_word_label(label) for label in test_label_list]\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "w_acc = np.mean(acc_list)\n",
    "w_acc_std = np.std(acc_list, ddof=1)\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word results:\\nAccuracy:{w_acc:6.4f} std:{w_acc_std:6.4f}\\n\")\n",
    "w_corr = np.mean(corr_list)\n",
    "w_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word corr results:\\nCorrect:{w_corr:6.4f} std:{w_corr_std:6.4f}\")\n",
    "\n",
    "print(\"-----------\")\n",
    "per_selection = (\n",
    "    grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"stim\"][\"SFFW\"]\n",
    "    / 60\n",
    "    * STIMULI_LABEL_INTERVAL_s\n",
    ")\n",
    "pp_array(\n",
    "    [\n",
    "        per_selection,\n",
    "        clf_acc,\n",
    "        clf_std,\n",
    "        l_acc,\n",
    "        l_acc_std,\n",
    "        l_corr,\n",
    "        l_corr_std,\n",
    "        w_acc,\n",
    "        w_acc_std,\n",
    "        w_corr,\n",
    "        w_corr_std,\n",
    "        stim_sffw_ITR,\n",
    "    ],\n",
    "    delimiter,\n",
    ")\n",
    "\n",
    "print(\"\\n=================stim_9 W2W grammar==================\")\n",
    "\n",
    "print(\"\\ngrammar viterbi:\")\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, aw2aw_stimulus_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, aw2aw_stimulus_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, aw2aw_stimulus_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    np.array2string(accuracy_list, max_line_width=120, precision=4, suppress_small=True)\n",
    ")\n",
    "clf_acc = np.mean(accuracy_list)\n",
    "clf_std = np.std(accuracy_list, ddof=1)\n",
    "print(f\"classification accuracy:\\n{clf_acc:6.4f}\\t{clf_std:6.4f}\")\n",
    "print(\"-----------\")\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "l_acc = np.mean(acc_list)\n",
    "l_acc_std = np.std(acc_list, ddof=1)\n",
    "print(f\"letter acc:\\n{l_acc:6.4f} std:{l_acc_std:6.4f}\\n\")\n",
    "l_corr = np.mean(corr_list)\n",
    "l_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"letter corr:\\n{l_corr:6.4f} std:{l_corr_std:6.4f}\")\n",
    "print(\"-----------\")\n",
    "test_label_word_list = [\n",
    "    letter_label_to_word_label(pred) for pred in aw2aw_stimulus_pred_y_list\n",
    "]\n",
    "pred_y_word_list = [letter_label_to_word_label(label) for label in test_label_list]\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "w_acc = np.mean(acc_list)\n",
    "w_acc_std = np.std(acc_list, ddof=1)\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word acc:\\n{w_acc:6.4f} std:{w_acc_std:6.4f}\\n\")\n",
    "w_corr = np.mean(corr_list)\n",
    "w_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word corr results:\\nCorrect:{w_corr:6.4f} std:{w_corr_std:6.4f}\")\n",
    "\n",
    "print(\"-----------\")\n",
    "per_selection = (\n",
    "    grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"stim\"][\"W2W\"]\n",
    "    / 60\n",
    "    * STIMULI_LABEL_INTERVAL_s\n",
    ")\n",
    "stim_sffw_ITR = grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"stim\"][\"W2W\"] * clf_acc\n",
    "pp_array(\n",
    "    [\n",
    "        per_selection,\n",
    "        clf_acc,\n",
    "        clf_std,\n",
    "        l_acc,\n",
    "        l_acc_std,\n",
    "        l_corr,\n",
    "        l_corr_std,\n",
    "        w_acc,\n",
    "        w_acc_std,\n",
    "        w_corr,\n",
    "        w_corr_std,\n",
    "        stim_sffw_ITR,\n",
    "    ],\n",
    "    delimiter,\n",
    ")\n",
    "\n",
    "print(\"\\n====================M&S_500 letter grammar====================\")\n",
    "\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(\n",
    "            test_label_list, mackenzie_soukoreff_pred_bigram_weighted_letter_label_list\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "bigram_acc = np.mean(accuracy_list)\n",
    "bigram_std = np.std(accuracy_list, ddof=1)\n",
    "print(f\"bigram:\\n{bigram_acc:6.4f}\\t{bigram_std:6.4f}\")\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(\n",
    "            test_label_list,\n",
    "            mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "letter_viterbi_mean_acc = np.mean(accuracy_list)\n",
    "letter_viterbi_mean_std = np.std(accuracy_list, ddof=1)\n",
    "print(\n",
    "    f\"letter viterbi accuracy: {letter_viterbi_mean_acc:6.4f}\\tstd:{letter_viterbi_mean_std:6.4f}\"\n",
    ")\n",
    "# info_per_select =  [information_transfer_per_selection(mackenzie_soukoreff_letter_prior_prob_dict, cm) for cm in mackenzie_soukoreff_letter_viterbi_cm_list]\n",
    "# cm = np.sum(mackenzie_soukoreff_letter_viterbi_cm_list, axis=0)\n",
    "cm_list = [\n",
    "    confusion_matrix(label, pred_y)\n",
    "    for label, pred_y in zip(\n",
    "        test_label_list,\n",
    "        mackenzie_soukoreff_pred_letter_viterbi_decode_letter_label_list,\n",
    "    )\n",
    "]\n",
    "cm = np.sum(cm_list, axis=0)\n",
    "naive_info_per_select = naive_information_transfer_per_selection(\n",
    "    num_label, letter_viterbi_mean_acc\n",
    ")\n",
    "better_info_per_select = information_transfer_per_selection(\n",
    "    grammar_info[\"stimulus_letter_prior_prob_dict\"], cm\n",
    ")\n",
    "naive_ITR = 60 / STIMULI_LABEL_INTERVAL_s * naive_info_per_select\n",
    "better_ITR = 60 / STIMULI_LABEL_INTERVAL_s * better_info_per_select\n",
    "print(\n",
    "    f\" Naive letter viterbi information transfer per selection: {naive_info_per_select:6.4f} ITR: {naive_ITR:6.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Better letter viterbi information transfer per selection: {better_info_per_select:6.4f} ITR: {better_ITR:6.4f}\"\n",
    ")\n",
    "print(\"-----------\")\n",
    "pp_array(\n",
    "    [\n",
    "        bigram_acc,\n",
    "        bigram_std,\n",
    "        letter_viterbi_mean_acc,\n",
    "        letter_viterbi_mean_std,\n",
    "        naive_ITR,\n",
    "        better_ITR,\n",
    "    ],\n",
    "    delimiter,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n====================M&S_500 SFFW grammar====================\")\n",
    "\n",
    "\n",
    "print(\"grammar viterbi:\")\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, mackenzie_soukoreff_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, mackenzie_soukoreff_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, mackenzie_soukoreff_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "clf_acc = np.mean(accuracy_list)\n",
    "clf_std = np.std(accuracy_list, ddof=1)\n",
    "print(\n",
    "    np.array2string(\n",
    "        np.array(accuracy_list), max_line_width=120, precision=4, suppress_small=True\n",
    "    )\n",
    ")\n",
    "print(f\"classification results:\\nAccuracy: {clf_acc:6.4f} std: {clf_std:6.4f}\")\n",
    "print(\"-----------\")\n",
    "l_acc = np.mean(acc_list)\n",
    "l_acc_std = np.std(acc_list, ddof=1)\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"letter results:\\nAccuracy:{l_acc:6.4f} std:{l_acc_std:6.4f}\\n\")\n",
    "l_corr = np.mean(corr_list)\n",
    "l_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"letter corr:\\n{l_acc_std:6.4f}\\t{l_corr:6.4f}\")\n",
    "print(\"-----------\")\n",
    "test_label_word_list = [\n",
    "    letter_label_to_word_label(pred) for pred in mackenzie_soukoreff_pred_y_list\n",
    "]\n",
    "pred_y_word_list = [letter_label_to_word_label(label) for label in test_label_list]\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "w_acc = np.mean(acc_list)\n",
    "w_acc_std = np.std(acc_list, ddof=1)\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word acc:\\n{w_acc:6.4f}\\t{w_acc_std:6.4f}\\n\")\n",
    "w_corr = np.mean(corr_list)\n",
    "w_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word corr:\\n{w_corr:6.4f}\\t{w_corr_std:6.4f}\")\n",
    "print(\"-----------\")\n",
    "\n",
    "per_selection = (\n",
    "    grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"MS\"][\"SFFW\"]\n",
    "    / 60\n",
    "    * STIMULI_LABEL_INTERVAL_s\n",
    ")\n",
    "stim_sffw_ITR = grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"MS\"][\"SFFW\"] * clf_acc\n",
    "\n",
    "pp_array(\n",
    "    [\n",
    "        per_selection,\n",
    "        clf_acc,\n",
    "        clf_std,\n",
    "        l_acc,\n",
    "        l_acc_std,\n",
    "        l_corr,\n",
    "        l_corr_std,\n",
    "        w_acc,\n",
    "        w_acc_std,\n",
    "        w_corr,\n",
    "        w_corr_std,\n",
    "        stim_sffw_ITR,\n",
    "    ],\n",
    "    delimiter,\n",
    ")\n",
    "\n",
    "print(\"\\n=================M&S_500 W2W grammar==================\")\n",
    "\n",
    "print(\"grammar viterbi:\")\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, aw2aw_mackenzie_soukoreff_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, aw2aw_mackenzie_soukoreff_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "accuracy_list = np.array(\n",
    "    [\n",
    "        accuracy_score(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_list, aw2aw_mackenzie_soukoreff_pred_y_list)\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    np.array2string(accuracy_list, max_line_width=120, precision=4, suppress_small=True)\n",
    ")\n",
    "clf_acc = np.mean(accuracy_list)\n",
    "clf_std = np.std(accuracy_list, ddof=1)\n",
    "print(\n",
    "    f\"classification accuracy:\\n{np.mean(accuracy_list):6.4f} std: {np.std(accuracy_list, ddof=1):6.4f}\"\n",
    ")\n",
    "print(\"-----------\")\n",
    "l_acc = np.mean(acc_list)\n",
    "l_acc_std = np.std(acc_list, ddof=1)\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"letter acc:\\n{clf_acc:6.4f}\\t{clf_std:6.4f}\\n\")\n",
    "l_corr = np.mean(corr_list)\n",
    "l_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"letter corr:\\n{l_corr:6.4f}\\t{l_corr_std:6.4f}\")\n",
    "print(\"-----------\")\n",
    "test_label_word_list = [\n",
    "    letter_label_to_word_label(pred) for pred in aw2aw_mackenzie_soukoreff_pred_y_list\n",
    "]\n",
    "pred_y_word_list = [letter_label_to_word_label(label) for label in test_label_list]\n",
    "acc_list = np.array(\n",
    "    [\n",
    "        tok_acc(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "corr_list = np.array(\n",
    "    [\n",
    "        tok_corr(label, pred_y)\n",
    "        for label, pred_y in zip(test_label_word_list, pred_y_word_list)\n",
    "    ]\n",
    ")\n",
    "w_acc = np.mean(acc_list)\n",
    "w_acc_std = np.std(acc_list, ddof=1)\n",
    "print(np.array2string(acc_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word acc:\\n{w_acc:6.4f}\\t{w_acc_std:6.4f}\\n\")\n",
    "w_corr = np.mean(corr_list)\n",
    "w_corr_std = np.std(corr_list, ddof=1)\n",
    "print(np.array2string(corr_list, max_line_width=120, precision=4, suppress_small=True))\n",
    "print(f\"word corr:\\n{w_corr:6.4f}\\t{w_corr_std:6.4f}\")\n",
    "print(\"-----------\")\n",
    "\n",
    "per_selection = (\n",
    "    grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"MS\"][\"W2W\"]\n",
    "    / 60\n",
    "    * STIMULI_LABEL_INTERVAL_s\n",
    ")\n",
    "stim_sffw_ITR = grammar_decode_ITR[STIMULI_LABEL_INTERVAL_s][\"MS\"][\"W2W\"] * clf_acc\n",
    "\n",
    "pp_array(\n",
    "    [\n",
    "        per_selection,\n",
    "        clf_acc,\n",
    "        clf_std,\n",
    "        l_acc,\n",
    "        l_acc_std,\n",
    "        l_corr,\n",
    "        l_corr_std,\n",
    "        w_acc,\n",
    "        w_acc_std,\n",
    "        w_corr,\n",
    "        w_corr_std,\n",
    "        stim_sffw_ITR,\n",
    "    ],\n",
    "    delimiter,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
